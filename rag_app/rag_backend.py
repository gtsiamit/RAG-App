from langchain.chat_models import init_chat_model
from langchain_openai import OpenAIEmbeddings
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from typing_extensions import List, TypedDict, Tuple
from langgraph.graph import StateGraph
from langchain_core.prompts import PromptTemplate
from langgraph.checkpoint.memory import MemorySaver
import textwrap
import faiss


class State(TypedDict):
    """
    Represents the state of the RAG system.
    This includes the question being asked, the context
    retrieved from the vector store, the answer
    generated by the model and the chat history.
    """

    question: str
    context: List[Document]
    answer: str
    chat_history: List[Tuple[str, str]]


class RAG:
    """
    A class that implements a Retrieval-Augmented Generation (RAG) system.
    """

    def __init__(
        self,
        thread_id: str,
        docs_path: str,
        chat_model: str = "gpt-4.1-nano-2025-04-14",
        embeddings_model: str = "text-embedding-3-small",
    ):
        """
        Initializes the RAG system with the given thread ID and document path.

        Args:
            thread_id (str): The thread ID for the conversation.
            docs_path (str): The path to the documents to be used for retrieval.
            chat_model (str, optional): The LLM chat model to be used. Defaults to "gpt-4.1-nano-2025-04-14".
            embeddings_model (str, optional): The embeddings model to be used. Defaults to "text-embedding-3-small".
        """

        # Store the thread ID in an instance variable
        self.thread_id = thread_id

        self.docs_path = docs_path

        # Configuration for the graph, including the thread ID
        self.config = {"configurable": {"thread_id": self.thread_id}}

        # Initialize the chat model with the specified model name
        self.llm_chat_model = init_chat_model(chat_model, model_provider="openai")

        # Initialize the embeddings model with the specified model name
        self.embeddings_model = OpenAIEmbeddings(model=embeddings_model)

        # Set up the prompt template for the RAG system
        self.prompt_template = self._prompt_template()

        # Load the documents from the specified path
        self.documents = self._load_docs()

        # Set up the vector store for document retrieval
        self.vector_store = self._setup_vector_store()

        # Store documents to vector db
        self.document_ids = self._store_docs_to_vector_db()

        # Build the graph for the RAG system
        self.graph = self._build_graph()

    def _get_embeddings_dim(self) -> int:
        """
        Retrieves the dimension of the embeddings used by the embeddings model.

        Returns:
            int: The dimension of the embeddings.
        """

        return len(
            self.embeddings_model.embed_query("This is to get the embedding dimension")
        )

    def _setup_vector_store(self) -> FAISS:
        """
        Sets up the vector store for document retrieval using FAISS.

        Returns:
            FAISS: An instance of the FAISS vector store initialized with the embeddings model.
        """

        # Get the dimension of the embeddings
        embeddings_dim = self._get_embeddings_dim()

        # Create a FAISS index for L2 distance with the specified embeddings dimension
        index = faiss.IndexFlatL2(embeddings_dim)

        # Initialize the FAISS vector store with the embeddings model and the index
        vector_store = FAISS(
            embedding_function=self.embeddings_model,
            index=index,
            docstore=InMemoryDocstore(),
            index_to_docstore_id={},
        )

        return vector_store

    def _load_docs(self) -> List[Document]:
        """
        Loads documents from the specified path using the PyPDFLoader.

        Returns:
            List[Document]: A list of Document objects loaded from the PDF file.
        """

        loader = PyPDFLoader(file_path=self.docs_path)

        documents = loader.load()
        print("documents", len(documents))

        return documents

    def _split_docs(self) -> List[Document]:
        """
        Splits the loaded documents into smaller chunks using a text splitter.

        Returns:
            List[Document]: A list of Document objects that have been split into smaller chunks.
        """

        # Create a text splitter to split documents into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            add_start_index=True,
        )

        # Split the loaded documents into smaller chunks
        doc_splits = text_splitter.split_documents(self.documents)
        print("doc_splits", len(doc_splits))

        return doc_splits

    def _store_docs_to_vector_db(self):
        """
        Stores the split documents into the vector store for retrieval.
        This method splits the documents into smaller chunks and adds them to the vector store.

        Returns:
            List[str]: A list of document IDs corresponding to the stored documents in the vector store.
        """

        docs_splits = self._split_docs()
        document_ids = self.vector_store.add_documents(documents=docs_splits)

        print("document_ids", len(document_ids))

        return document_ids

    @staticmethod
    def _prompt_template() -> PromptTemplate:
        """
        Creates a prompt template for the RAG system.

        Returns:
            PromptTemplate: A PromptTemplate object with the conversation history, context, and question.
        """

        prompt_template = textwrap.dedent(
            """\
        You are a useful assistant.
        Use the following pieces of context to answer the question at the end.
        The conversation so far is also given to you.
        If you don't know the answer, just say that you don't know, don't try to make up an answer.
        Use three sentences maximum and keep the answer as concise as possible.

        Conversation so far:
        {history}

        Context:
        {context}

        Question: {question}

        Answer:"""
        )

        prompt = PromptTemplate.from_template(prompt_template)

        return prompt

    def _retrieve(self, state: State, k: int = 5) -> State:
        """
        Retrieves relevant documents from the vector store based on the question in the state.
        This method performs a similarity search in the vector store using the question from the state.
        It returns the state with the retrieved documents added to the context.

        Args:
            state (State): The current state containing the question.
            k (int, optional): The number of documents to retrieve. Defaults to 5.

        Returns:
            State: The updated state with the retrieved documents added to the context.
        """

        retrieved_docs = self.vector_store.similarity_search(state["question"], k=k)

        return {**state, "context": retrieved_docs}

    @staticmethod
    def _join_context(context: List[Document]) -> str:
        """
        Joins the content of the retrieved documents into a single string.

        Args:
            context (List[Document]): A list of Document objects containing the retrieved context.

        Returns:
            str: A string containing the joined content of the documents.
        """

        context_joined = "\n\n".join(doc.page_content for doc in context)

        return context_joined

    @staticmethod
    def _get_history_text(history: List[Tuple[str, str]]) -> str:
        """
        Converts the chat history into a formatted string.

        Args:
            history (List[Tuple[str, str]]): A list of tuples where each tuple contains a question and its corresponding answer.

        Returns:
            str: A formatted string representing the chat history, limited to the last three exchanges.
        """

        # Limit the history to the last three exchanges
        trimmed_history = history[-3:]

        history_text = "\n".join(
            f"Question: {question}\nAnswer: {answer}"
            for question, answer in trimmed_history
        )

        return history_text

    def _build_message_prompt(self, question: str, context: str, history: str) -> str:
        """
        Builds a message prompt using the question, context, and chat history.

        Args:
            question (str): The question to be answered by the model.
            context (str): The context retrieved from the vector store.
            history (str): The chat history formatted as a string.

        Returns:
            str: The constructed prompt ready to be sent to the LLM.
        """

        prompt = self.prompt_template.invoke(
            {"question": question, "context": context, "history": history}
        )

        return prompt

    @staticmethod
    def _update_chat_history(
        history: List[Tuple[str, str]], q: str, a: str
    ) -> List[Tuple[str, str]]:
        """
        Updates the chat history with the latest question and answer.

        Args:
            history (List[Tuple[str, str]]): The current chat history as a list of tuples.
            q (str): The latest question asked by the user.
            a (str): The answer provided by the model.

        Returns:
            List[Tuple[str, str]]: The updated chat history including the latest question and answer.
        """

        return history + [(q, a)]

    def _generate(self, state: State) -> State:
        """
        Generates an answer to the question in the state using the LLM chat model.

        Args:
            state (State): The current state containing the question, context, and chat history.

        Returns:
            State: The updated state with the generated answer and updated chat history.
        """

        question = state["question"]
        context = state["context"]
        chat_history = state["chat_history"]

        # Join the context documents into a single string for the prompt
        context_for_prompt = self._join_context(context)

        # Get the chat history as a formatted string
        chat_history_text = self._get_history_text(chat_history)

        # Build the message prompt using the question, context, and chat history
        message = self._build_message_prompt(
            question=question, context=context_for_prompt, history=chat_history_text
        )

        # Invoke the LLM chat model with the constructed message prompt
        response = self.llm_chat_model.invoke(message)

        # Update the chat history with the latest question and answer
        updated_history = self._update_chat_history(
            history=chat_history, q=question, a=response.content
        )

        return {**state, "answer": response.content, "chat_history": updated_history}

    def _build_graph(self) -> StateGraph:
        """
        Builds the state graph for the RAG system, defining the sequence of operations.

        Returns:
            StateGraph: A compiled state graph that defines the sequence of operations for the RAG system.
        """

        # Create a state graph to define the sequence of operations in the RAG system
        graph_builder = StateGraph(State)

        # Add nodes to the graph for retrieval and generation
        # These nodes will be responsible for retrieving context from the vector store
        # and generating an answer based on the retrieved context and the question
        graph_builder.add_node("retrieve", self._retrieve)
        graph_builder.add_node("generate", self._generate)

        # Set the entry point of the graph to the "retrieve" node
        # This tells the graph where to start executing the operations
        graph_builder.set_entry_point("retrieve")

        # Add an edge from the "retrieve" node to the "generate" node
        # This defines the flow of operations, where after retrieval, the
        # system will proceed to generate an answer
        # This edge connects the retrieval step to the generation step
        graph_builder.add_edge("retrieve", "generate")

        # Define memory for the graph to save state between invocations
        memory = MemorySaver()

        return graph_builder.compile(checkpointer=memory)

    def invoke(self, input_question: str, first_call: bool = False) -> str:
        """
        Invokes the RAG system with the given input question.

        Args:
            input_question (str): The question to be answered by the RAG system.
            first_call (bool, optional): A flag indicating if this is the first call to the system. Defaults to False.

        Returns:
            str: The answer generated by the RAG system based on the input question and the context retrieved from the vector store.
        """

        # If this is the first call, initialize the state with an empty chat history
        if first_call:
            response = self.graph.invoke(
                input={"question": input_question, "chat_history": []},
                config=self.config,
            )
        # If this is not the first call, use the existing state to invoke the graph
        else:
            response = self.graph.invoke(
                input={"question": input_question}, config=self.config
            )

        return response["answer"]
